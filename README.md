# ğŸ›’ Retail Data Engineering Project

An end-to-end modular data pipeline built using **PySpark + PostgreSQL**.

This project demonstrates:
- CSV ingestion using PySpark
- Data cleansing
- Automated table creation in PostgreSQL
- JDBC-based data loading
- Modular project architecture
- Environment-based configuration
- Git-ready professional structure

---

## ğŸ— Project Architecture

```
CSV Files â†’ PySpark â†’ Data Cleaning â†’ PostgreSQL (via JDBC)
```

---

## ğŸ“‚ Project Structure

```
retail_data_project/
â”‚
â”œâ”€â”€ data/                   # Raw CSV files
â”œâ”€â”€ spark_jars/             # PostgreSQL JDBC driver
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ spark_session.py    # Spark session creator
â”‚   â”œâ”€â”€ db.py               # PostgreSQL connection & query execution
â”‚   â”œâ”€â”€ loader.py           # Generic JDBC loader
â”‚
â”œâ”€â”€ tables/
â”‚   â”œâ”€â”€ orders.py           # Orders pipeline logic
â”‚   â”œâ”€â”€ customers.py        # (Future)
â”‚   â”œâ”€â”€ products.py         # (Future)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py         # Environment config handler
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ spark_utils.py      # Utility functions
â”‚
â”œâ”€â”€ run_pipeline.py         # Main pipeline runner
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Tech Stack

- Python
- PySpark (Local Mode)
- PostgreSQL
- JDBC
- psycopg2
- VS Code
- Git

---

## ğŸ—„ Database Schema (Orders Table)

```sql
CREATE TABLE orders (
    order_id INT PRIMARY KEY,
    order_date TIMESTAMP,
    order_customer_id INT,
    order_status VARCHAR(50)
);
```

---

## ğŸ” Environment Variables (.env)

Create a `.env` file in root:

```
ORDERS_FILE_PATH=your_csv_path_here

DB_HOST=localhost
DB_USER=postgres
DB_PASS=your_password
DB_NAME=retail_db
DB_PORT=5432
```

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-repo-url>
cd retail_data_project
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv py-venv
py-venv\Scripts\activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Download PostgreSQL JDBC Driver

Download from:
https://jdbc.postgresql.org/download/

Place the `.jar` file inside:

```
spark_jars/
```

---

## ğŸš€ Running the Pipeline

Run using:

```bash
spark-submit --jars spark_jars/postgresql-42.x.x.jar run_pipeline.py
```

---

## âœ… What This Project Demonstrates

- Modular pipeline design
- Separation of concerns
- Reusable Spark session management
- Automated DDL execution
- Secure config using environment variables
- Clean JDBC integration
- Scalable design for multiple tables

---

## ğŸ“ˆ Future Enhancements

- Incremental data loading
- SCD Type-2 implementation
- Batchsize optimization for JDBC
- Airflow orchestration
- AWS S3 integration
- Terraform-based infrastructure
- Logging & monitoring
- Unit testing with pytest

---

## ğŸ‘¨â€ğŸ’» Author

Retail Data Engineering Practice Project  
Built for hands-on learning and cloud-ready development.

---

## ğŸ§  Learning Goal

This project is designed to simulate real-world Data Engineering workflows and enforce production-level best practices.
